import numpy
import scipy.stats
import lightgbm
import logging

logger = logging.getLogger(__name__)


def manual_bin(x, y, cuts):
    """
    The function discretizes the x vector and then summarizes over the y vector
    based on the discretization result.

    Parameters:
      x    : A numeric vector to discretize without missing values,
             e.g. numpy.nan or math.nan
      y    : A numeric vector with binary values of 0/1 and with the same length
             of x
      cuts : A list of numeric values as cut points to discretize x.

    Returns:
      A list of dictionaries for the binning outcome.

    Example:
        from sklearn.datasets import make_classification
        X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_classes=2, random_state=42)
        X = X.flatten()
        cuts = [-0.5, 0, 0.5]

        for x in manual_bin(X, y, cuts):
            print(x)

    # {'bin': 1, 'freq': 436, 'miss': 0, 'bads': np.int64(226), 'minx': np.float64(-4.543441462057407), 'maxx': np.float64(-0.5076584767190596)}
    # {'bin': 2, 'freq': 104, 'miss': 0, 'bads': np.int64(49), 'minx': np.float64(-0.4977201080203948), 'maxx': np.float64(-0.00016253650167286082)}
    # {'bin': 3, 'freq': 106, 'miss': 0, 'bads': np.int64(43), 'minx': np.float64(0.00305478623466493), 'maxx': np.float64(0.48853934246646313)}
    # {'bin': 4, 'freq': 354, 'miss': 0, 'bads': np.int64(184), 'minx': np.float64(0.5017155926595585), 'maxx': np.float64(3.909761909687836)}
    """

    _x = [_ for _ in x]
    _y = [_ for _ in y]
    _c = sorted([_ for _ in set(cuts)] + [-numpy.inf, numpy.inf])
    _g = numpy.searchsorted(_c, _x).tolist()

    _l1 = sorted(zip(_g, _x, _y), key=lambda x: x[0])
    # logger.debug(f"{_l1=} {_c=} {_g=}")
    _l2 = zip(set(_g), [[l for l in _l1 if l[0] == g] for g in set(_g)])

    return sorted(
        [
            dict(
                zip(
                    ["bin", "freq", "miss", "bads", "minx", "maxx"],
                    [
                        _1,
                        len(_2),
                        0,
                        sum([_[2] for _ in _2]),
                        min([_[1] for _ in _2]),
                        max([_[1] for _ in _2]),
                    ],
                )
            )
            for _1, _2 in _l2
        ],
        key=lambda x: x["bin"],
    )


def miss_bin(y):
    """
    The function summarizes the y vector with binary values of 0/1 and is not
    supposed to be called directly by users.

    Parameters:
      y : A numeric vector with binary values of 0/1.

    Returns:
      A dictionary.
    """

    return {
        "bin": 0,
        "freq": len([_ for _ in y]),
        "miss": len([_ for _ in y]),
        "bads": sum([_ for _ in y]),
        "minx": numpy.nan,
        "maxx": numpy.nan,
    }


def add_miss(d, l):
    """
    The function appends missing value category, if any, to the binning outcome
    and is an utility function and is not supposed to be called directly by
    the user.

    Parameters:
      d : A list with lists generated by input vectors of binning functions.
      l : A list of dicts.

    Returns:
      A list of dicts.
    """

    _l = l[:]

    if len([_ for _ in d if _[2] == 0]) > 0:
        _m = miss_bin([_[1] for _ in d if _[2] == 0])
        if _m["bads"] == 0:
            for _ in ["freq", "miss", "bads"]:
                _l[0][_] = _l[0][_] + _m[_]
        elif _m["freq"] == _m["bads"]:
            for _ in ["freq", "miss", "bads"]:
                _l[-1][_] = _l[-1][_] + _m[_]
        else:
            _l.append(_m)

    return _l


def gen_rule(tbl, pts):
    """
    The function generates binning rules based on the binning outcome table and
    a list of cut points and is an utility function that is not supposed to be
    called directly by users.

    Parameters:
      tbl : A intermediate table of the binning outcome within each binning
            function
      pts : A list cut points for the binning

    Returns:
      A list of dictionaries with binning rules
    """

    for _ in tbl:
        if _["bin"] == 0:
            _["rule"] = "numpy.isnan($X$)"
        elif _["bin"] == len(pts) + 1:
            if _["miss"] == 0:
                _["rule"] = "$X$ > " + str(pts[-1])
            else:
                _["rule"] = "$X$ > " + str(pts[-1]) + " or numpy.isnan($X$)"
        elif _["bin"] == 1:
            if _["miss"] == 0:
                _["rule"] = "$X$ <= " + str(pts[0])
            else:
                _["rule"] = "$X$ <= " + str(pts[0]) + " or numpy.isnan($X$)"
        else:
            _["rule"] = (
                "$X$ > "
                + str(pts[_["bin"] - 2])
                + " and $X$ <= "
                + str(pts[_["bin"] - 1])
            )

    _sel = ["bin", "freq", "miss", "bads", "rate", "woe", "iv", "ks", "rule"]

    return [{k: _[k] for k in _sel} for _ in tbl]


def gen_woe(x):
    """
    The function calculates weight of evidence and information value based on the
    binning outcome within each binning function and is an utility function that
    is not supposed to be called directly by users.

    Parameters:
      x : A list of dictionaries for the binning outcome.

    Returns:
      A list of dictionaries with additional keys to the input.
    """

    _freq = sum(_["freq"] for _ in x)
    _bads = sum(_["bads"] for _ in x)

    _l1 = sorted(
        [
            {
                **_,
                "rate": round(_["bads"] / _["freq"], 4),
                "woe": round(
                    numpy.log(
                        (_["bads"] / _bads)
                        / ((_["freq"] - _["bads"]) / (_freq - _bads))
                    ),
                    4,
                ),
                "iv": round(
                    (_["bads"] / _bads - (_["freq"] - _["bads"]) / (_freq - _bads))
                    * numpy.log(
                        (_["bads"] / _bads)
                        / ((_["freq"] - _["bads"]) / (_freq - _bads))
                    ),
                    4,
                ),
            }
            for _ in x
        ],
        key=lambda _x: _x["bin"],
    )

    cumsum = lambda x: [sum([_ for _ in x][0 : (i + 1)]) for i in range(len(x))]

    _cumb = cumsum([_["bads"] / _bads for _ in _l1])
    _cumg = cumsum([(_["freq"] - _["bads"]) / (_freq - _bads) for _ in _l1])
    _ks = [round(numpy.abs(_[0] - _[1]) * 100, 2) for _ in zip(_cumb, _cumg)]

    return [{**_1, "ks": _2} for _1, _2 in zip(_l1, _ks)]


def gbm_bin(x, y):
    """
    The function discretizes the x vector based on the gradient boosting machine
    and then summarizes over the y vector to derive the weight of evidence
    transformaton (WoE) and information values.

    Parameters:
      x : A numeric vector to discretize. It is a list, 1-D numpy array,
          or pandas series.
      y : A numeric vector with binary values of 0/1 and with the same length
          of x. It is a list, 1-D numpy array, or pandas series.

    Returns:
      A dictionary with two keys:
        "cut" : A numeric vector with cut points applied to the x vector.
        "tbl" : A list of dictionaries summarizing the binning outcome.

    Example:
      gbm_bin(derog, bad)["cut"]
      # [1.0, 2.0, 3.0, 22.0, 26.0]

      view_bin(gbm_bin(derog, bad))
      |  bin  |   freq |   miss |   bads |   rate |     woe |     iv |    ks |                     rule                      |
      |-------|--------|--------|--------|--------|---------|--------|-------|-----------------------------------------------|
      |   0   |    213 |    213 |     70 | 0.3286 |  0.6416 | 0.0178 |  2.77 | numpy.isnan($X$)                              |
      |   1   |   3741 |      0 |    560 | 0.1497 | -0.3811 | 0.0828 | 18.95 | $X$ <= 1.0                                    |
      |   2   |    478 |      0 |    121 | 0.2531 |  0.2740 | 0.0066 | 16.52 | $X$ > 1.0 and $X$ <= 2.0                      |
      |   3   |    332 |      0 |     86 | 0.2590 |  0.3050 | 0.0058 | 14.63 | $X$ > 2.0 and $X$ <= 3.0                      |
      |   4   |   1063 |      0 |    353 | 0.3321 |  0.6572 | 0.0934 |  0.42 | $X$ > 3.0 and $X$ <= 22.0                     |
      |   5   |      6 |      0 |      3 | 0.5000 |  1.3559 | 0.0025 |  0.23 | $X$ > 22.0 and $X$ <= 26.0                    |
      |   6   |      4 |      0 |      3 | 0.7500 |  2.4546 | 0.0056 |  0.00 | $X$ > 26.0                                    |
    """

    _data = [_ for _ in zip(x, y, ~numpy.isnan(x))]

    _x = [_[0] for _ in _data if _[2] == 1]
    _y = [_[1] for _ in _data if _[2] == 1]

    _cor = scipy.stats.spearmanr(_x, _y)[0]
    _con = "1" if _cor > 0 else "-1"

    _gbm = lightgbm.LGBMRegressor(
        num_leaves=100,
        min_child_samples=3,
        n_estimators=1,
        random_state=1,
        monotone_constraints=_con,
    )
    _gbm.fit(numpy.reshape(_x, [-1, 1]), _y)

    _f = numpy.abs(_gbm.predict(numpy.reshape(_x, [-1, 1])))

    _l1 = sorted(list(zip(_f, _x, _y)), key=lambda x: x[0])

    _l2 = [[l for l in _l1 if l[0] == f] for f in sorted(set(_f))]

    _l3 = [
        [
            *set(_[0] for _ in l),
            max(_[1] for _ in l),
            numpy.mean([_[2] for _ in l]),
            sum(_[2] for _ in l),
        ]
        for l in _l2
    ]

    _c = sorted([_[1] for _ in [l for l in _l3 if l[2] < 1 and l[2] > 0 and l[3] > 1]])

    _p = _c[1:-1] if len(_c) > 2 else _c[:-1]

    _l4 = sorted(manual_bin(_x, _y, _p), key=lambda x: x["bads"] / x["freq"])

    _l5 = add_miss(_data, _l4)

    return {"cut": _p, "tbl": gen_rule(gen_woe(_l5), _p)}
